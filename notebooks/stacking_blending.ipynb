{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c00",
   "metadata": {},
   "source": [
    "# Stacking & Blending Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-26 11:30:14.324\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mPROJ_ROOT path is: C:\\Users\\admin\\Documents\\1WBS\\energy_prices\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI: sqlite:///C:/Users/admin/Documents/1WBS/energy_prices/models/mlflow.db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import ElasticNet, HuberRegressor, Lasso, Ridge\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from src.config import MLFLOW_TRACKING_URI, DATA_DIR\n",
    "from src.features.preprocessors import load_dataset\n",
    "from src.modeling.metrics import calculate_metrics\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "print('MLflow tracking URI:', MLFLOW_TRACKING_URI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split:  train=80%  |  OOF=15%  |  test=5%\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "\n",
    "BASE_MODEL_RUN_IDS = [\n",
    "    'd091f1c8342b4c649f86931157f07617',\n",
    "    'd8deb4fefd374fbfbd0fe86ab0c61fc9',\n",
    "    'ecbb6e460ff3430fb9bb5411ebd09200',\n",
    "    'fc43d42dd7ea45cd827823ade033d091',\n",
    "    'daa9083db9664b42979f1793543e642f',\n",
    "    'cdde162175474a468a8894876a96ca61',\n",
    "    '31b03ec78a17495a8f00e19c03a3335b',\n",
    "    'b1dfd4b6ba58496fa6c3449b59de21dc'\n",
    "]\n",
    "\n",
    "EXPERIMENT_NAME = 'ensemble_stacking'\n",
    "\n",
    "# Three-way split fractions (train = 1 − OOF − test)\n",
    "ENSEMBLE_TEST_FRACTION = 0.05\n",
    "ENSEMBLE_OOF_FRACTION  = 0.15\n",
    "\n",
    "# Trailing window for time-varying blend weights\n",
    "BLEND_ROLLING_WINDOW = 24 * 150  # hours\n",
    "\n",
    "# Meta-learner hyperparameters\n",
    "META_RIDGE_ALPHA       = 1.0\n",
    "META_LASSO_ALPHA       = 1.0\n",
    "META_ENET_ALPHA        = 1.0\n",
    "META_ENET_L1_RATIO     = 0.5\n",
    "META_HUBER_EPSILON     = 1.35\n",
    "META_LGBM_N_ESTIMATORS = 100\n",
    "META_LGBM_LR           = 0.05\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "train_frac = 1 - ENSEMBLE_OOF_FRACTION - ENSEMBLE_TEST_FRACTION\n",
    "print(f'Split:  train={train_frac:.0%}  |  OOF={ENSEMBLE_OOF_FRACTION:.0%}  |  test={ENSEMBLE_TEST_FRACTION:.0%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03",
   "metadata": {},
   "source": [
    "## Load base models from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/26 11:30:15 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/26 11:30:15 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a818fdb0c3435ab2f7132c170779a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7011b98a82ad4331b5dc26daf3b7e063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-26 11:30:15.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.features.preprocessors\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mLoading dataset from run_id=9be233856ef149d193d72d77536d1534\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547e76fa416d409b84d3b5c7dc34f5f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-26 11:30:16.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.features.preprocessors\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mLoaded dataset: X=(4055, 159), y=(4055, 24)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d091f1c8: MultiOutputRegressor, gs=1, X=(4055, 159)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1735d6ead3846da9db6551192d78c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b00923410a40e9af397b972750445e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-26 11:30:16.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.features.preprocessors\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mLoading dataset from run_id=9be233856ef149d193d72d77536d1534\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeea362a89fe4ab1839ce06c5b7c1c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-26 11:30:16.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.features.preprocessors\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mLoaded dataset: X=(4055, 159), y=(4055, 24)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d8deb4fe: MultiOutputRegressor, gs=1, X=(4055, 159)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc5199988d94fc7996a16f8ea6888b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd01feaec93141269c78ec0785089e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-26 11:30:17.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.features.preprocessors\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mLoading dataset from run_id=247ccd98f2614cc5aa0bf834c1f1835e\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf2e7d87af14af9b2b9fccc2dbdae63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-26 11:30:18.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.features.preprocessors\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mLoaded dataset: X=(97320, 85), y=(97320, 1)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecbb6e46: XGBRegressor, gs=24, X=(97320, 85)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f93b0115b9d4845b1e4560695deeba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029bad76cb404b3981985cdf002dc43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-26 11:30:18.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.features.preprocessors\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mLoading dataset from run_id=247ccd98f2614cc5aa0bf834c1f1835e\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af5506ed5c043c8b9a4c6510e7fb494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-26 11:30:19.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.features.preprocessors\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mLoaded dataset: X=(97320, 85), y=(97320, 1)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc43d42d: XGBRegressor, gs=24, X=(97320, 85)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100142990b2e4da6878964bff0cb237c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f0e66273504a12b327590b3f86bd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-26 11:30:19.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.features.preprocessors\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mLoading dataset from run_id=8f6fb5a7566d456eae9e3ba24e7e3a9e\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d4cc0c3d98450592afbe12ba92fe88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-26 11:30:20.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.features.preprocessors\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mLoaded dataset: X=(97320, 61), y=(97320, 1)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daa9083d: CatBoostRegressor, gs=24, X=(97320, 61)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19c85f79fd0436e97c6e753fe61175a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34690ce6a2dc494a8a16f4a964579c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-26 11:30:20.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.features.preprocessors\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mLoading dataset from run_id=93706fa242c44a10acda359c4042c19c\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88457837fd4e407689344f762e417b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-26 11:30:21.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.features.preprocessors\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mLoaded dataset: X=(97320, 30), y=(97320, 1)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cdde1621: CatBoostRegressor, gs=24, X=(97320, 30)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986d2a72e6bd402bb41184083c0793e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ca93dfa4c8420dab6223633c5a1d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-26 11:30:21.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.features.preprocessors\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mLoading dataset from run_id=8f6fb5a7566d456eae9e3ba24e7e3a9e\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828ce5740d234351b61287f20171b44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-26 11:30:22.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.features.preprocessors\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mLoaded dataset: X=(97320, 61), y=(97320, 1)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31b03ec7: LGBMRegressor, gs=24, X=(97320, 61)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9421be353abe4b629f9f06471449cbe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e97fd0f55b4cd48b4c5d20f18b921f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-26 11:30:23.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.features.preprocessors\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mLoading dataset from run_id=93706fa242c44a10acda359c4042c19c\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885d5cab39b34999983dce3d9a6f25c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-26 11:30:23.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.features.preprocessors\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mLoaded dataset: X=(97320, 30), y=(97320, 1)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1dfd4b6: LGBMRegressor, gs=24, X=(97320, 30)\n",
      "\n",
      "Loaded 8 base models.\n"
     ]
    }
   ],
   "source": [
    "base_models = []\n",
    "\n",
    "for run_id in BASE_MODEL_RUN_IDS:\n",
    "    run = mlflow.get_run(run_id)\n",
    "    params = run.data.params\n",
    "\n",
    "    dataset_run_id = params['dataset_run_id']\n",
    "    group_size     = int(params.get('group_size', 1))\n",
    "    model_class    = params['model_class']\n",
    "\n",
    "    fitted_pipeline = mlflow.sklearn.load_model(f'runs:/{run_id}/model')\n",
    "    X, y_df, meta = load_dataset(run_id=dataset_run_id)\n",
    "    y_array = y_df.values\n",
    "\n",
    "    base_models.append({\n",
    "        'run_id':           run_id,\n",
    "        'model_class':      model_class,\n",
    "        'group_size':       group_size,\n",
    "        'fitted_pipeline':  fitted_pipeline,\n",
    "        'X':                X,\n",
    "        'y_array':          y_array,\n",
    "        'y_df':             y_df,\n",
    "        'index':            X.index,\n",
    "    })\n",
    "    print(f'{run_id[:8]}: {model_class}, gs={group_size}, X={X.shape}')\n",
    "\n",
    "print(f'\\nLoaded {len(base_models)} base models.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05",
   "metadata": {},
   "source": [
    "## Prediction flattening helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built y_series_full for 8 models, length=97309\n"
     ]
    }
   ],
   "source": [
    "def flatten_predictions(preds, index, group_size):\n",
    "    \"\"\"Convert model predictions to a hourly pd.Series.\"\"\"\n",
    "    if isinstance(preds, np.ndarray) and preds.ndim == 2 and preds.shape[1] == 1:\n",
    "        preds = preds.ravel()\n",
    "\n",
    "    if group_size == 24:\n",
    "        return pd.Series(preds, index=index)\n",
    "\n",
    "    elif group_size == 1:\n",
    "        if not (isinstance(preds, np.ndarray) and preds.ndim == 2\n",
    "                and preds.shape[1] == 24):\n",
    "            raise ValueError(\n",
    "                f'Expected (N_days, 24) for group_size=1, '\n",
    "                f'got shape {getattr(preds, \"shape\", type(preds))}'\n",
    "            )\n",
    "        hourly_timestamps = []\n",
    "        hourly_values = []\n",
    "        for i, day in enumerate(index):\n",
    "            for h in range(24):\n",
    "                hourly_timestamps.append(day + pd.Timedelta(hours=h))\n",
    "                hourly_values.append(preds[i, h])\n",
    "        ts_index = pd.DatetimeIndex(hourly_timestamps)\n",
    "        if ts_index.tz is None:\n",
    "            ts_index = ts_index.tz_localize('UTC')\n",
    "        return pd.Series(hourly_values, index=ts_index)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Unexpected group_size={group_size}. Expected 1 or 24.')\n",
    "\n",
    "\n",
    "def build_full_y_series(model_info):\n",
    "    \"\"\"Build a full hourly y Series from a base model's dataset.\"\"\"\n",
    "    y_df       = model_info['y_df']\n",
    "    index      = model_info['index']\n",
    "    group_size = model_info['group_size']\n",
    "\n",
    "    if group_size == 24:\n",
    "        return pd.Series(y_df.iloc[:, 0].values, index=index)\n",
    "    else:\n",
    "        return flatten_predictions(y_df.values, index, group_size=1)\n",
    "\n",
    "\n",
    "for model_info in base_models:\n",
    "    model_info['y_series_full'] = build_full_y_series(model_info)\n",
    "    ys = model_info['y_series_full']\n",
    "    if ys.index.duplicated().any():\n",
    "        model_info['y_series_full'] = ys[~ys.index.duplicated(keep='first')]\n",
    "\n",
    "print(f'Built y_series_full for {len(base_models)} models, '\n",
    "      f'length={len(base_models[0][\"y_series_full\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07",
   "metadata": {},
   "source": [
    "## Generate OOF and test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d091f1c8: OOF=14614h, test=4872h\n",
      "d8deb4fe: OOF=14614h, test=4872h\n",
      "ecbb6e46: OOF=14590h, test=4872h\n",
      "fc43d42d: OOF=14590h, test=4872h\n",
      "daa9083d: OOF=14590h, test=4872h\n",
      "cdde1621: OOF=14590h, test=4872h\n",
      "31b03ec7: OOF=14590h, test=4872h\n",
      "b1dfd4b6: OOF=14590h, test=4872h\n"
     ]
    }
   ],
   "source": [
    "for model_info in base_models:\n",
    "    run_id          = model_info['run_id']\n",
    "    group_size      = model_info['group_size']\n",
    "    X               = model_info['X']\n",
    "    y_array         = model_info['y_array']\n",
    "    fitted_pipeline = model_info['fitted_pipeline']\n",
    "\n",
    "    # Three-way split (group-aligned boundaries)\n",
    "    oof_start_idx  = int(len(X) * (1 - ENSEMBLE_OOF_FRACTION - ENSEMBLE_TEST_FRACTION))\n",
    "    oof_start_idx  = (oof_start_idx // group_size) * group_size\n",
    "\n",
    "    test_start_idx = int(len(X) * (1 - ENSEMBLE_TEST_FRACTION))\n",
    "    test_start_idx = (test_start_idx // group_size) * group_size\n",
    "\n",
    "    X_train = X.iloc[:oof_start_idx]\n",
    "    X_oof   = X.iloc[oof_start_idx:test_start_idx]\n",
    "    X_test  = X.iloc[test_start_idx:]\n",
    "\n",
    "    y_train = y_array[:oof_start_idx]\n",
    "    y_oof   = y_array[oof_start_idx:test_start_idx]\n",
    "\n",
    "    model_info['X_train'] = X_train\n",
    "    model_info['X_oof']   = X_oof\n",
    "    model_info['X_test']  = X_test\n",
    "\n",
    "    # OOF predictions: clone pipeline, refit on train only, predict on OOF\n",
    "    cloned = clone(fitted_pipeline)\n",
    "    cloned.fit(X_train, y_train)\n",
    "    oof_preds  = cloned.predict(X_oof)\n",
    "    oof_series = flatten_predictions(oof_preds, X_oof.index, group_size)\n",
    "    if oof_series.index.duplicated().any():\n",
    "        oof_series = oof_series[~oof_series.index.duplicated(keep='first')]\n",
    "\n",
    "    # Test predictions: original pipeline (trained on full training set)\n",
    "    test_preds  = fitted_pipeline.predict(X_test)\n",
    "    test_series = flatten_predictions(test_preds, X_test.index, group_size)\n",
    "    if test_series.index.duplicated().any():\n",
    "        test_series = test_series[~test_series.index.duplicated(keep='first')]\n",
    "\n",
    "    model_info['oof_series']  = oof_series\n",
    "    model_info['test_series'] = test_series\n",
    "\n",
    "    print(f'{run_id[:8]}: OOF={len(oof_series)}h, test={len(test_series)}h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09",
   "metadata": {},
   "source": [
    "## Align predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "528f4737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate check done.\n"
     ]
    }
   ],
   "source": [
    "for m in base_models:\n",
    "    for key in ('oof_series', 'test_series'):\n",
    "        s = m[key]\n",
    "        n_dup = s.index.duplicated().sum()\n",
    "        if n_dup > 0:\n",
    "            print(f\"  {m['run_id'][:8]} {key}: dropping {n_dup} duplicate timestamp(s)\")\n",
    "            m[key] = s[~s.index.duplicated(keep='first')]\n",
    "\n",
    "print('Duplicate check done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF aligned : (14589, 8)  2023-11-22 23:00:00+00:00 -> 2025-07-22 21:00:00+00:00\n",
      "Test aligned: (4871, 8)  2025-07-22 22:00:00+00:00 -> 2026-02-10 22:00:00+00:00\n",
      "\n",
      "y_oof_true : (14589,),  NaNs: 0\n",
      "y_test_true: (4871,), NaNs: 0\n"
     ]
    }
   ],
   "source": [
    "def _to_utc(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalise a Series to UTC and drop any remaining duplicate timestamps.\"\"\"\n",
    "    if s.index.tz is not None:\n",
    "        s = s.tz_convert('UTC')\n",
    "    else:\n",
    "        s = s.tz_localize('UTC')\n",
    "    if s.index.duplicated().any():\n",
    "        s = s[~s.index.duplicated(keep='first')]\n",
    "    return s\n",
    "\n",
    "\n",
    "# Normalise all prediction and target series to UTC before alignment.\n",
    "# Hourly models carry Europe/Berlin (CET/CEST), daily-pivot models may produce\n",
    "# a different tz object from Timedelta arithmetic; mixing them confuses pandas.\n",
    "for m in base_models:\n",
    "    m['oof_series']    = _to_utc(m['oof_series'])\n",
    "    m['test_series']   = _to_utc(m['test_series'])\n",
    "    m['y_series_full'] = _to_utc(m['y_series_full'])\n",
    "\n",
    "# Inner join: keep only rows where ALL models have OOF predictions\n",
    "oof_df = pd.DataFrame(\n",
    "    {m['run_id']: m['oof_series'] for m in base_models}\n",
    ").dropna()\n",
    "\n",
    "# Inner join for test predictions\n",
    "test_df = pd.DataFrame(\n",
    "    {m['run_id']: m['test_series'] for m in base_models}\n",
    ").dropna()\n",
    "\n",
    "print(f'OOF aligned : {oof_df.shape}  '\n",
    "      f'{oof_df.index[0]} -> {oof_df.index[-1]}')\n",
    "print(f'Test aligned: {test_df.shape}  '\n",
    "      f'{test_df.index[0]} -> {test_df.index[-1]}')\n",
    "\n",
    "\n",
    "def get_actual_prices_at(timestamps):\n",
    "    \"\"\"Return actual energy prices (EUR/MWh) at the requested hourly timestamps.\"\"\"\n",
    "    sorted_models = sorted(\n",
    "        base_models, key=lambda m: len(m['y_series_full']), reverse=True\n",
    "    )\n",
    "    for m in sorted_models:\n",
    "        aligned = m['y_series_full'].reindex(timestamps)\n",
    "        if aligned.isna().sum() == 0:\n",
    "            return aligned.values\n",
    "\n",
    "    # Fallback: mean across all models' y series\n",
    "    combined = pd.concat(\n",
    "        [m['y_series_full'] for m in base_models], axis=1\n",
    "    ).mean(axis=1)\n",
    "    result = combined.reindex(timestamps)\n",
    "    n_missing = int(result.isna().sum())\n",
    "    if n_missing > 0:\n",
    "        print(f'Warning: {n_missing} timestamps have no actual price.')\n",
    "    return result.values\n",
    "\n",
    "\n",
    "y_oof_true  = get_actual_prices_at(oof_df.index)\n",
    "y_test_true = get_actual_prices_at(test_df.index)\n",
    "\n",
    "print(f'\\ny_oof_true : {y_oof_true.shape},  NaNs: {np.isnan(y_oof_true).sum()}')\n",
    "print(f'y_test_true: {y_test_true.shape}, NaNs: {np.isnan(y_test_true).sum()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11",
   "metadata": {},
   "source": [
    "## Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaky RMSE blend: {'rmse': '18.2913', 'mae': '10.6756', 'me': '-0.6924', 'r2': '0.8416'}\n",
      "Leaky MAE  blend: {'rmse': '18.2102', 'mae': '10.5312', 'me': '-0.8310', 'r2': '0.8430'}\n"
     ]
    }
   ],
   "source": [
    "n_models = len(base_models)\n",
    "\n",
    "# Combine OOF + test for leaky upper-bound weights\n",
    "combined_df     = pd.concat([oof_df, test_df])\n",
    "y_combined_true = np.concatenate([y_oof_true, y_test_true])\n",
    "\n",
    "_x0     = np.ones(n_models) / n_models\n",
    "_bounds = [(0, 1)] * n_models\n",
    "_cons   = [{'type': 'eq', 'fun': lambda w: w.sum() - 1}]\n",
    "\n",
    "\n",
    "def make_rmse_obj(vals, y):\n",
    "    def _obj(w):\n",
    "        return np.sqrt(np.mean(((vals * w).sum(axis=1) - y) ** 2))\n",
    "    return _obj\n",
    "\n",
    "\n",
    "def make_mae_obj(vals, y):\n",
    "    def _obj(w):\n",
    "        return np.mean(np.abs((vals * w).sum(axis=1) - y))\n",
    "    return _obj\n",
    "\n",
    "\n",
    "# RMSE-optimised (leaky)\n",
    "res = minimize(make_rmse_obj(combined_df.values, y_combined_true),\n",
    "               x0=_x0, method='SLSQP', bounds=_bounds, constraints=_cons)\n",
    "blend_weights     = res.x\n",
    "y_blend_pred      = (test_df.values * blend_weights).sum(axis=1)\n",
    "blend_metrics     = calculate_metrics(y_test_true, y_blend_pred)\n",
    "\n",
    "# MAE-optimised (leaky)\n",
    "res_mae = minimize(make_mae_obj(combined_df.values, y_combined_true),\n",
    "                   x0=_x0, method='SLSQP', bounds=_bounds, constraints=_cons)\n",
    "blend_weights_mae = res_mae.x\n",
    "y_blend_mae_pred  = (test_df.values * blend_weights_mae).sum(axis=1)\n",
    "blend_mae_metrics = calculate_metrics(y_test_true, y_blend_mae_pred)\n",
    "\n",
    "print('Leaky RMSE blend:', {k: f'{v:.4f}' for k, v in blend_metrics.items()})\n",
    "print('Leaky MAE  blend:', {k: f'{v:.4f}' for k, v in blend_mae_metrics.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l1bt1idn05k",
   "metadata": {},
   "source": [
    "### Leakage-free blend (OOF only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mgjhk97zom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LF RMSE — OOF: {'rmse': '24.7070', 'mae': '13.4467', 'me': '3.1905', 'r2': '0.7894'}\n",
      "LF RMSE — Test: {'rmse': '18.7589', 'mae': '11.0095', 'me': '-0.6307', 'r2': '0.8334'}\n",
      "LF MAE  — OOF: {'rmse': '24.8655', 'mae': '13.2310', 'me': '2.7210', 'r2': '0.7867'}\n",
      "LF MAE  — Test: {'rmse': '18.4340', 'mae': '10.7368', 'me': '-0.6292', 'r2': '0.8391'}\n"
     ]
    }
   ],
   "source": [
    "_x0_lf   = np.ones(n_models) / n_models\n",
    "_bnd_lf  = [(0, 1)] * n_models\n",
    "_cons_lf = [{'type': 'eq', 'fun': lambda w: w.sum() - 1}]\n",
    "\n",
    "\n",
    "def blend_oof_rmse(weights):\n",
    "    return np.sqrt(np.mean(((oof_df.values * weights).sum(axis=1) - y_oof_true) ** 2))\n",
    "\n",
    "\n",
    "def blend_oof_mae(weights):\n",
    "    return np.mean(np.abs((oof_df.values * weights).sum(axis=1) - y_oof_true))\n",
    "\n",
    "\n",
    "# RMSE-optimised (leakage-free)\n",
    "res_lf = minimize(blend_oof_rmse, x0=_x0_lf, method='SLSQP',\n",
    "                  bounds=_bnd_lf, constraints=_cons_lf)\n",
    "blend_weights_lf     = res_lf.x\n",
    "y_blend_lf_pred      = (test_df.values * blend_weights_lf).sum(axis=1)\n",
    "blend_lf_metrics     = calculate_metrics(y_test_true, y_blend_lf_pred)\n",
    "blend_lf_oof_metrics = calculate_metrics(y_oof_true,\n",
    "                                          (oof_df.values * blend_weights_lf).sum(axis=1))\n",
    "\n",
    "# MAE-optimised (leakage-free)\n",
    "res_lf_mae = minimize(blend_oof_mae, x0=_x0_lf, method='SLSQP',\n",
    "                      bounds=_bnd_lf, constraints=_cons_lf)\n",
    "blend_weights_lf_mae     = res_lf_mae.x\n",
    "y_blend_lf_mae_pred      = (test_df.values * blend_weights_lf_mae).sum(axis=1)\n",
    "blend_lf_mae_metrics     = calculate_metrics(y_test_true, y_blend_lf_mae_pred)\n",
    "blend_lf_mae_oof_metrics = calculate_metrics(y_oof_true,\n",
    "                                              (oof_df.values * blend_weights_lf_mae).sum(axis=1))\n",
    "\n",
    "print('LF RMSE — OOF:', {k: f'{v:.4f}' for k, v in blend_lf_oof_metrics.items()})\n",
    "print('LF RMSE — Test:', {k: f'{v:.4f}' for k, v in blend_lf_metrics.items()})\n",
    "print('LF MAE  — OOF:', {k: f'{v:.4f}' for k, v in blend_lf_mae_oof_metrics.items()})\n",
    "print('LF MAE  — Test:', {k: f'{v:.4f}' for k, v in blend_lf_mae_metrics.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n2w11b8b6sg",
   "metadata": {},
   "source": [
    "### Time-varying blend (recent OOF performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4jwx4nymlf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-varying blend (150-day window): {'rmse': '18.0223', 'mae': '10.2924', 'me': '-1.3482', 'r2': '0.8462'}\n"
     ]
    }
   ],
   "source": [
    "# Inverse-MAE weights from trailing OOF window\n",
    "window      = min(BLEND_ROLLING_WINDOW, len(oof_df))\n",
    "oof_tail    = oof_df.iloc[-window:]\n",
    "y_tail_true = y_oof_true[-window:]\n",
    "\n",
    "recent_mae       = np.abs(oof_tail.values - y_tail_true[:, np.newaxis]).mean(axis=0)\n",
    "inv_mae          = 1.0 / (recent_mae + 1e-6)\n",
    "blend_weights_tv = inv_mae / inv_mae.sum()\n",
    "\n",
    "y_blend_tv_pred  = (test_df.values * blend_weights_tv).sum(axis=1)\n",
    "blend_tv_metrics = calculate_metrics(y_test_true, y_blend_tv_pred)\n",
    "\n",
    "print(f'Time-varying blend ({window/24:.0f}-day window):',\n",
    "      {k: f'{v:.4f}' for k, v in blend_tv_metrics.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x78efahqxy8",
   "metadata": {},
   "source": [
    "### Greedy model addition (time-varying blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79nul5veins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ranking by OOF MAE (150-day window):\n",
      "\n",
      "  1. MultiOutputRegressor(d091f1c8)                MAE=14.5427\n",
      "  2. CatBoostRegressor(cdde1621)                   MAE=15.5628\n",
      "  3. LGBMRegressor(b1dfd4b6)                       MAE=16.1601\n",
      "  4. CatBoostRegressor(daa9083d)                   MAE=16.4702\n",
      "  5. MultiOutputRegressor(d8deb4fe)                MAE=16.5434\n",
      "  6. LGBMRegressor(31b03ec7)                       MAE=17.1583\n",
      "  7. XGBRegressor(ecbb6e46)                        MAE=17.7814\n",
      "  8. XGBRegressor(fc43d42d)                        MAE=19.6132\n",
      "\n",
      "Greedy addition (test-set metrics):\n",
      "  N  Added model                                        RMSE       MAE        R2\n",
      "--------------------------------------------------------------------------------\n",
      "  1  MultiOutputRegressor(d091f1c8)                  21.4958   13.4450    0.7812\n",
      "  2  CatBoostRegressor(cdde1621)                     18.3617   10.9633    0.8404\n",
      "  3  LGBMRegressor(b1dfd4b6)                         17.8550   10.4536    0.8491\n",
      "  4  CatBoostRegressor(daa9083d)                     17.7962   10.3446    0.8501\n",
      "  5  MultiOutputRegressor(d8deb4fe)                  18.1404   10.4776    0.8442\n",
      "  6  LGBMRegressor(31b03ec7)                         17.9275   10.3417    0.8478\n",
      "  7  XGBRegressor(ecbb6e46)                          17.8436   10.2202    0.8493\n",
      "  8  XGBRegressor(fc43d42d)                          18.0223   10.2924    0.8462\n"
     ]
    }
   ],
   "source": [
    "# Per-model OOF MAE over the trailing window\n",
    "model_oof_mae = {}\n",
    "for m in base_models:\n",
    "    rid = m['run_id']\n",
    "    mae = np.abs(oof_tail[rid].values - y_tail_true).mean()\n",
    "    model_oof_mae[rid] = mae\n",
    "\n",
    "# Sort models best (lowest MAE) to worst\n",
    "ranked_ids = sorted(model_oof_mae, key=model_oof_mae.get)\n",
    "\n",
    "# Map run_id -> short label\n",
    "labels = {m['run_id']: f\"{m['model_class']}({m['run_id'][:8]})\" for m in base_models}\n",
    "\n",
    "print(f'Model ranking by OOF MAE ({window/24:.0f}-day window):\\n')\n",
    "for i, rid in enumerate(ranked_ids, 1):\n",
    "    print(f'  {i}. {labels[rid]:45s} MAE={model_oof_mae[rid]:.4f}')\n",
    "\n",
    "# Incrementally add models and compute time-varying blend\n",
    "print(f'\\nGreedy addition (test-set metrics):')\n",
    "print(f'{\"N\":>3}  {\"Added model\":45s}  {\"RMSE\":>8}  {\"MAE\":>8}  {\"R2\":>8}')\n",
    "print('-' * 80)\n",
    "\n",
    "for n in range(1, len(ranked_ids) + 1):\n",
    "    subset_ids = ranked_ids[:n]\n",
    "    oof_sub = oof_tail[subset_ids].values\n",
    "    test_sub = test_df[subset_ids].values\n",
    "    y_sub = y_tail_true\n",
    "\n",
    "    sub_mae = np.abs(oof_sub - y_sub[:, np.newaxis]).mean(axis=0)\n",
    "    sub_inv = 1.0 / (sub_mae + 1e-6)\n",
    "    sub_w = sub_inv / sub_inv.sum()\n",
    "\n",
    "    y_pred = (test_sub * sub_w).sum(axis=1)\n",
    "    m = calculate_metrics(y_test_true, y_pred)\n",
    "    added = labels[ranked_ids[n - 1]]\n",
    "    print(f'{n:3d}  {added:45s}  {m[\"rmse\"]:8.4f}  {m[\"mae\"]:8.4f}  {m[\"r2\"]:8.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: {'rmse': '18.9769', 'mae': '11.7342', 'me': '-4.7112', 'r2': '0.8295'}\n",
      "Lasso: {'rmse': '19.1526', 'mae': '11.8759', 'me': '-5.8581', 'r2': '0.8263'}\n",
      "ElasticNet: {'rmse': '19.1692', 'mae': '11.8988', 'me': '-5.9209', 'r2': '0.8260'}\n",
      "Huber: {'rmse': '19.6825', 'mae': '11.8917', 'me': '-7.5049', 'r2': '0.8166'}\n",
      "LightGBM: {'rmse': '20.4502', 'mae': '12.2794', 'me': '-5.1741', 'r2': '0.8020'}\n"
     ]
    }
   ],
   "source": [
    "stacking_results = {}\n",
    "\n",
    "# Ridge (positive=True available since sklearn 1.1)\n",
    "try:\n",
    "    ridge_meta = Ridge(alpha=META_RIDGE_ALPHA, positive=True)\n",
    "    ridge_meta.fit(oof_df.values, y_oof_true)\n",
    "except TypeError:\n",
    "    ridge_meta = Ridge(alpha=META_RIDGE_ALPHA)\n",
    "    ridge_meta.fit(oof_df.values, y_oof_true)\n",
    "\n",
    "y_ridge_pred  = ridge_meta.predict(test_df.values)\n",
    "ridge_metrics = calculate_metrics(y_test_true, y_ridge_pred)\n",
    "stacking_results['Stack (Ridge)'] = {\n",
    "    'metrics': ridge_metrics, 'model': ridge_meta, 'predictions': y_ridge_pred,\n",
    "}\n",
    "print('Ridge:', {k: f'{v:.4f}' for k, v in ridge_metrics.items()})\n",
    "\n",
    "# Lasso\n",
    "lasso_meta = Lasso(alpha=META_LASSO_ALPHA, max_iter=10_000)\n",
    "lasso_meta.fit(oof_df.values, y_oof_true)\n",
    "\n",
    "y_lasso_pred  = lasso_meta.predict(test_df.values)\n",
    "lasso_metrics = calculate_metrics(y_test_true, y_lasso_pred)\n",
    "stacking_results['Stack (Lasso)'] = {\n",
    "    'metrics': lasso_metrics, 'model': lasso_meta, 'predictions': y_lasso_pred,\n",
    "}\n",
    "print('Lasso:', {k: f'{v:.4f}' for k, v in lasso_metrics.items()})\n",
    "\n",
    "# ElasticNet\n",
    "enet_meta = ElasticNet(alpha=META_ENET_ALPHA, l1_ratio=META_ENET_L1_RATIO, max_iter=10_000)\n",
    "enet_meta.fit(oof_df.values, y_oof_true)\n",
    "\n",
    "y_enet_pred  = enet_meta.predict(test_df.values)\n",
    "enet_metrics = calculate_metrics(y_test_true, y_enet_pred)\n",
    "stacking_results['Stack (ElasticNet)'] = {\n",
    "    'metrics': enet_metrics, 'model': enet_meta, 'predictions': y_enet_pred,\n",
    "}\n",
    "print('ElasticNet:', {k: f'{v:.4f}' for k, v in enet_metrics.items()})\n",
    "\n",
    "# Huber (robust to price spikes)\n",
    "huber_meta = HuberRegressor(epsilon=META_HUBER_EPSILON, max_iter=500)\n",
    "huber_meta.fit(oof_df.values, y_oof_true)\n",
    "\n",
    "y_huber_pred  = huber_meta.predict(test_df.values)\n",
    "huber_metrics = calculate_metrics(y_test_true, y_huber_pred)\n",
    "stacking_results['Stack (Huber)'] = {\n",
    "    'metrics': huber_metrics, 'model': huber_meta, 'predictions': y_huber_pred,\n",
    "}\n",
    "print('Huber:', {k: f'{v:.4f}' for k, v in huber_metrics.items()})\n",
    "\n",
    "# LightGBM (shallow trees to limit overfitting on OOF)\n",
    "lgbm_meta = LGBMRegressor(\n",
    "    n_estimators=META_LGBM_N_ESTIMATORS,\n",
    "    learning_rate=META_LGBM_LR,\n",
    "    max_depth=3,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=-1,\n",
    ")\n",
    "lgbm_meta.fit(oof_df.values, y_oof_true)\n",
    "\n",
    "y_lgbm_pred  = lgbm_meta.predict(test_df.values)\n",
    "lgbm_metrics = calculate_metrics(y_test_true, y_lgbm_pred)\n",
    "stacking_results['Stack (LightGBM)'] = {\n",
    "    'metrics': lgbm_metrics, 'model': lgbm_meta, 'predictions': y_lgbm_pred,\n",
    "}\n",
    "print('LightGBM:', {k: f'{v:.4f}' for k, v in lgbm_metrics.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15",
   "metadata": {},
   "source": [
    "## Results comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_bd45f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_bd45f_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_bd45f_level0_col1\" class=\"col_heading level0 col1\" >RMSE</th>\n",
       "      <th id=\"T_bd45f_level0_col2\" class=\"col_heading level0 col2\" >MAE</th>\n",
       "      <th id=\"T_bd45f_level0_col3\" class=\"col_heading level0 col3\" >R2</th>\n",
       "      <th id=\"T_bd45f_level0_col4\" class=\"col_heading level0 col4\" >Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_bd45f_row0_col0\" class=\"data row0 col0\" >Blend time-varying</td>\n",
       "      <td id=\"T_bd45f_row0_col1\" class=\"data row0 col1\" >18.02</td>\n",
       "      <td id=\"T_bd45f_row0_col2\" class=\"data row0 col2\" >10.29</td>\n",
       "      <td id=\"T_bd45f_row0_col3\" class=\"data row0 col3\" >0.8462</td>\n",
       "      <td id=\"T_bd45f_row0_col4\" class=\"data row0 col4\" >ensemble test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_bd45f_row1_col0\" class=\"data row1 col0\" >Blend MAE (leaky)</td>\n",
       "      <td id=\"T_bd45f_row1_col1\" class=\"data row1 col1\" >18.21</td>\n",
       "      <td id=\"T_bd45f_row1_col2\" class=\"data row1 col2\" >10.53</td>\n",
       "      <td id=\"T_bd45f_row1_col3\" class=\"data row1 col3\" >0.8430</td>\n",
       "      <td id=\"T_bd45f_row1_col4\" class=\"data row1 col4\" >leakage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_bd45f_row2_col0\" class=\"data row2 col0\" >Blend RMSE (leaky)</td>\n",
       "      <td id=\"T_bd45f_row2_col1\" class=\"data row2 col1\" >18.29</td>\n",
       "      <td id=\"T_bd45f_row2_col2\" class=\"data row2 col2\" >10.68</td>\n",
       "      <td id=\"T_bd45f_row2_col3\" class=\"data row2 col3\" >0.8416</td>\n",
       "      <td id=\"T_bd45f_row2_col4\" class=\"data row2 col4\" >leakage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_bd45f_row3_col0\" class=\"data row3 col0\" >Blend MAE (LF)</td>\n",
       "      <td id=\"T_bd45f_row3_col1\" class=\"data row3 col1\" >18.43</td>\n",
       "      <td id=\"T_bd45f_row3_col2\" class=\"data row3 col2\" >10.74</td>\n",
       "      <td id=\"T_bd45f_row3_col3\" class=\"data row3 col3\" >0.8391</td>\n",
       "      <td id=\"T_bd45f_row3_col4\" class=\"data row3 col4\" >ensemble test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_bd45f_row4_col0\" class=\"data row4 col0\" >Blend RMSE (LF)</td>\n",
       "      <td id=\"T_bd45f_row4_col1\" class=\"data row4 col1\" >18.76</td>\n",
       "      <td id=\"T_bd45f_row4_col2\" class=\"data row4 col2\" >11.01</td>\n",
       "      <td id=\"T_bd45f_row4_col3\" class=\"data row4 col3\" >0.8334</td>\n",
       "      <td id=\"T_bd45f_row4_col4\" class=\"data row4 col4\" >ensemble test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_bd45f_row5_col0\" class=\"data row5 col0\" >Stack (Ridge)</td>\n",
       "      <td id=\"T_bd45f_row5_col1\" class=\"data row5 col1\" >18.98</td>\n",
       "      <td id=\"T_bd45f_row5_col2\" class=\"data row5 col2\" >11.73</td>\n",
       "      <td id=\"T_bd45f_row5_col3\" class=\"data row5 col3\" >0.8295</td>\n",
       "      <td id=\"T_bd45f_row5_col4\" class=\"data row5 col4\" >ensemble test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_bd45f_row6_col0\" class=\"data row6 col0\" >Stack (Lasso)</td>\n",
       "      <td id=\"T_bd45f_row6_col1\" class=\"data row6 col1\" >19.15</td>\n",
       "      <td id=\"T_bd45f_row6_col2\" class=\"data row6 col2\" >11.88</td>\n",
       "      <td id=\"T_bd45f_row6_col3\" class=\"data row6 col3\" >0.8263</td>\n",
       "      <td id=\"T_bd45f_row6_col4\" class=\"data row6 col4\" >ensemble test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_bd45f_row7_col0\" class=\"data row7 col0\" >Stack (ElasticNet)</td>\n",
       "      <td id=\"T_bd45f_row7_col1\" class=\"data row7 col1\" >19.17</td>\n",
       "      <td id=\"T_bd45f_row7_col2\" class=\"data row7 col2\" >11.90</td>\n",
       "      <td id=\"T_bd45f_row7_col3\" class=\"data row7 col3\" >0.8260</td>\n",
       "      <td id=\"T_bd45f_row7_col4\" class=\"data row7 col4\" >ensemble test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_bd45f_row8_col0\" class=\"data row8 col0\" >Stack (Huber)</td>\n",
       "      <td id=\"T_bd45f_row8_col1\" class=\"data row8 col1\" >19.68</td>\n",
       "      <td id=\"T_bd45f_row8_col2\" class=\"data row8 col2\" >11.89</td>\n",
       "      <td id=\"T_bd45f_row8_col3\" class=\"data row8 col3\" >0.8166</td>\n",
       "      <td id=\"T_bd45f_row8_col4\" class=\"data row8 col4\" >ensemble test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_bd45f_row9_col0\" class=\"data row9 col0\" >LGBMRegressor (b1dfd4b6)</td>\n",
       "      <td id=\"T_bd45f_row9_col1\" class=\"data row9 col1\" >20.06</td>\n",
       "      <td id=\"T_bd45f_row9_col2\" class=\"data row9 col2\" >11.87</td>\n",
       "      <td id=\"T_bd45f_row9_col3\" class=\"data row9 col3\" >0.8452</td>\n",
       "      <td id=\"T_bd45f_row9_col4\" class=\"data row9 col4\" >own test period</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_bd45f_row10_col0\" class=\"data row10 col0\" >Stack (LightGBM)</td>\n",
       "      <td id=\"T_bd45f_row10_col1\" class=\"data row10 col1\" >20.45</td>\n",
       "      <td id=\"T_bd45f_row10_col2\" class=\"data row10 col2\" >12.28</td>\n",
       "      <td id=\"T_bd45f_row10_col3\" class=\"data row10 col3\" >0.8020</td>\n",
       "      <td id=\"T_bd45f_row10_col4\" class=\"data row10 col4\" >ensemble test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_bd45f_row11_col0\" class=\"data row11 col0\" >XGBRegressor (ecbb6e46)</td>\n",
       "      <td id=\"T_bd45f_row11_col1\" class=\"data row11 col1\" >20.49</td>\n",
       "      <td id=\"T_bd45f_row11_col2\" class=\"data row11 col2\" >11.95</td>\n",
       "      <td id=\"T_bd45f_row11_col3\" class=\"data row11 col3\" >0.8385</td>\n",
       "      <td id=\"T_bd45f_row11_col4\" class=\"data row11 col4\" >own test period</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_bd45f_row12_col0\" class=\"data row12 col0\" >LGBMRegressor (31b03ec7)</td>\n",
       "      <td id=\"T_bd45f_row12_col1\" class=\"data row12 col1\" >20.52</td>\n",
       "      <td id=\"T_bd45f_row12_col2\" class=\"data row12 col2\" >12.04</td>\n",
       "      <td id=\"T_bd45f_row12_col3\" class=\"data row12 col3\" >0.8380</td>\n",
       "      <td id=\"T_bd45f_row12_col4\" class=\"data row12 col4\" >own test period</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_bd45f_row13_col0\" class=\"data row13 col0\" >CatBoostRegressor (cdde1621)</td>\n",
       "      <td id=\"T_bd45f_row13_col1\" class=\"data row13 col1\" >20.56</td>\n",
       "      <td id=\"T_bd45f_row13_col2\" class=\"data row13 col2\" >12.45</td>\n",
       "      <td id=\"T_bd45f_row13_col3\" class=\"data row13 col3\" >0.8374</td>\n",
       "      <td id=\"T_bd45f_row13_col4\" class=\"data row13 col4\" >own test period</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_bd45f_row14_col0\" class=\"data row14 col0\" >CatBoostRegressor (daa9083d)</td>\n",
       "      <td id=\"T_bd45f_row14_col1\" class=\"data row14 col1\" >21.04</td>\n",
       "      <td id=\"T_bd45f_row14_col2\" class=\"data row14 col2\" >12.40</td>\n",
       "      <td id=\"T_bd45f_row14_col3\" class=\"data row14 col3\" >0.8298</td>\n",
       "      <td id=\"T_bd45f_row14_col4\" class=\"data row14 col4\" >own test period</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_bd45f_row15_col0\" class=\"data row15 col0\" >XGBRegressor (fc43d42d)</td>\n",
       "      <td id=\"T_bd45f_row15_col1\" class=\"data row15 col1\" >22.25</td>\n",
       "      <td id=\"T_bd45f_row15_col2\" class=\"data row15 col2\" >13.07</td>\n",
       "      <td id=\"T_bd45f_row15_col3\" class=\"data row15 col3\" >0.8096</td>\n",
       "      <td id=\"T_bd45f_row15_col4\" class=\"data row15 col4\" >own test period</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_bd45f_row16_col0\" class=\"data row16 col0\" >MultiOutputRegressor (d091f1c8)</td>\n",
       "      <td id=\"T_bd45f_row16_col1\" class=\"data row16 col1\" >24.76</td>\n",
       "      <td id=\"T_bd45f_row16_col2\" class=\"data row16 col2\" >14.35</td>\n",
       "      <td id=\"T_bd45f_row16_col3\" class=\"data row16 col3\" >0.7747</td>\n",
       "      <td id=\"T_bd45f_row16_col4\" class=\"data row16 col4\" >own test period</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd45f_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_bd45f_row17_col0\" class=\"data row17 col0\" >MultiOutputRegressor (d8deb4fe)</td>\n",
       "      <td id=\"T_bd45f_row17_col1\" class=\"data row17 col1\" >26.23</td>\n",
       "      <td id=\"T_bd45f_row17_col2\" class=\"data row17 col2\" >14.92</td>\n",
       "      <td id=\"T_bd45f_row17_col3\" class=\"data row17 col3\" >0.7470</td>\n",
       "      <td id=\"T_bd45f_row17_col4\" class=\"data row17 col4\" >own test period</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1881e302e40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "# Base model results (from MLflow, their own test period)\n",
    "for m in base_models:\n",
    "    run = mlflow.get_run(m['run_id'])\n",
    "    rm  = run.data.metrics\n",
    "    rows.append({\n",
    "        'Model': f\"{m['model_class']} ({m['run_id'][:8]})\",\n",
    "        'RMSE': rm.get('rmse', float('nan')),\n",
    "        'MAE':  rm.get('mae',  float('nan')),\n",
    "        'R2':   rm.get('r2',   float('nan')),\n",
    "        'Note': 'own test period',\n",
    "    })\n",
    "\n",
    "# Leaky blends (OOF + test)\n",
    "for label, metrics in [\n",
    "    ('Blend RMSE (leaky)',  blend_metrics),\n",
    "    ('Blend MAE (leaky)',   blend_mae_metrics),\n",
    "]:\n",
    "    rows.append({'Model': label, 'RMSE': metrics['rmse'], 'MAE': metrics['mae'],\n",
    "                 'R2': metrics['r2'], 'Note': 'leakage'})\n",
    "\n",
    "# Leakage-free blends\n",
    "for label, metrics in [\n",
    "    ('Blend RMSE (LF)',     blend_lf_metrics),\n",
    "    ('Blend MAE (LF)',      blend_lf_mae_metrics),\n",
    "    ('Blend time-varying',  blend_tv_metrics),\n",
    "]:\n",
    "    rows.append({'Model': label, 'RMSE': metrics['rmse'], 'MAE': metrics['mae'],\n",
    "                 'R2': metrics['r2'], 'Note': 'ensemble test'})\n",
    "\n",
    "# Stacking\n",
    "for name, res in stacking_results.items():\n",
    "    mm = res['metrics']\n",
    "    rows.append({'Model': name, 'RMSE': mm['rmse'], 'MAE': mm['mae'],\n",
    "                 'R2': mm['r2'], 'Note': 'ensemble test'})\n",
    "\n",
    "results_df = pd.DataFrame(rows).sort_values('RMSE').reset_index(drop=True)\n",
    "results_df.style.format({'RMSE': '{:.2f}', 'MAE': '{:.2f}', 'R2': '{:.4f}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17",
   "metadata": {},
   "source": [
    "## Log to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged ensemble_blending_rmse: bf4afc365286479eae3b73acd02c9045\n",
      "Logged ensemble_blending_mae: fb5f14360c414738927347cc38fa393e\n",
      "Logged ensemble_blending_lf: a70d1828087644b3ba1fd54a959a14ef\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/26 11:34:02 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged ensemble_blending_lf_mae: 18366c669d88416fba6701fedee6ea9f\n",
      "Logged ensemble_blending_tv: 2231e492c3b5462fab8df0908dcdddab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/26 11:34:17 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Stack (Ridge): d39463ae5b1e4f2f92e4bb11fcf7bf61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/26 11:34:24 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Stack (Lasso): d9afc48f1f7743599647ecaa5637f439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/26 11:34:30 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Stack (ElasticNet): 4c52187a311f46f1bf578828bc036e17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/26 11:34:36 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Stack (Huber): 84c2fc31e0d2425d9cb64e5f65b7833c\n",
      "Logged Stack (LightGBM): 638f3e6370c7435090c97876e57e7492\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "base_run_ids_str       = ','.join(BASE_MODEL_RUN_IDS)\n",
    "base_model_classes_str = ','.join([m['model_class'] for m in base_models])\n",
    "\n",
    "_split_params = {\n",
    "    'ensemble_train_fraction': 1 - ENSEMBLE_OOF_FRACTION - ENSEMBLE_TEST_FRACTION,\n",
    "    'ensemble_oof_fraction':   ENSEMBLE_OOF_FRACTION,\n",
    "    'ensemble_test_fraction':  ENSEMBLE_TEST_FRACTION,\n",
    "    'n_base_models':           len(BASE_MODEL_RUN_IDS),\n",
    "    'base_model_run_ids':      base_run_ids_str,\n",
    "}\n",
    "\n",
    "\n",
    "def _log_npy_artifact(arr, filename, subdir='predictions'):\n",
    "    with tempfile.NamedTemporaryFile(suffix='.npy', delete=False) as f:\n",
    "        tmp = f.name\n",
    "    np.save(tmp, arr)\n",
    "    mlflow.log_artifact(tmp, artifact_path=subdir)\n",
    "    os.unlink(tmp)\n",
    "\n",
    "\n",
    "# Log blending runs\n",
    "blend_run_specs = [\n",
    "    ('ensemble_blending_rmse',   'blending_leaky',  'OOF+test', 'RMSE',     blend_metrics,         blend_weights,        y_blend_pred),\n",
    "    ('ensemble_blending_mae',    'blending_leaky',  'OOF+test', 'MAE',      blend_mae_metrics,     blend_weights_mae,    y_blend_mae_pred),\n",
    "    ('ensemble_blending_lf',     'blending_lf',     'OOF',      'RMSE',     blend_lf_metrics,      blend_weights_lf,     y_blend_lf_pred),\n",
    "    ('ensemble_blending_lf_mae', 'blending_lf',     'OOF',      'MAE',      blend_lf_mae_metrics,  blend_weights_lf_mae, y_blend_lf_mae_pred),\n",
    "    ('ensemble_blending_tv',     'blending_tv_lf',  'OOF_tail', 'inv_MAE',  blend_tv_metrics,      blend_weights_tv,     y_blend_tv_pred),\n",
    "]\n",
    "\n",
    "for run_name, method_tag, weight_set, objective, metrics, weights, preds in blend_run_specs:\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        mlflow.log_params({\n",
    "            **_split_params,\n",
    "            'method':                  'blending',\n",
    "            'objective':               objective,\n",
    "            'weight_optimisation_set': weight_set,\n",
    "            'blend_rolling_window':    BLEND_ROLLING_WINDOW if 'tv' in run_name else 'N/A',\n",
    "        })\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.set_tags({'model_type': 'ensemble', 'method': method_tag,\n",
    "                         'base_models': base_model_classes_str})\n",
    "        mlflow.log_dict({'weights': weights.tolist(), 'run_ids': BASE_MODEL_RUN_IDS},\n",
    "                        f'{run_name}_weights.json')\n",
    "        _log_npy_artifact(preds, f'{run_name}_predictions.npy')\n",
    "        print(f'Logged {run_name}: {run.info.run_id}')\n",
    "\n",
    "# Log stacking runs\n",
    "meta_extra_params = {\n",
    "    'Stack (Ridge)':      {'meta_ridge_alpha':    META_RIDGE_ALPHA},\n",
    "    'Stack (Lasso)':      {'meta_lasso_alpha':    META_LASSO_ALPHA},\n",
    "    'Stack (ElasticNet)': {'meta_enet_alpha':     META_ENET_ALPHA,\n",
    "                           'meta_enet_l1_ratio':  META_ENET_L1_RATIO},\n",
    "    'Stack (Huber)':      {'meta_huber_epsilon':  META_HUBER_EPSILON},\n",
    "    'Stack (LightGBM)':   {'meta_lgbm_n_estimators': META_LGBM_N_ESTIMATORS,\n",
    "                           'meta_lgbm_lr':            META_LGBM_LR,\n",
    "                           'meta_lgbm_max_depth':     3},\n",
    "}\n",
    "\n",
    "for method_name, res in stacking_results.items():\n",
    "    meta_learner = method_name.split('(')[1].rstrip(')')\n",
    "    run_name     = f'ensemble_stack_{meta_learner.lower()}'\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        params = {**_split_params, 'method': 'stacking', 'meta_learner': meta_learner}\n",
    "        params.update(meta_extra_params.get(method_name, {}))\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics(res['metrics'])\n",
    "        mlflow.set_tags({'model_type': 'ensemble', 'method': 'stacking',\n",
    "                         'base_models': base_model_classes_str})\n",
    "        mlflow.sklearn.log_model(res['model'], 'meta_model')\n",
    "        _log_npy_artifact(res['predictions'], f'{run_name}_predictions.npy')\n",
    "        print(f'Logged {method_name}: {run.info.run_id}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
